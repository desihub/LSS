#LSS to run for SV3, after a new spec release 
#Ashley just git pulls from the LSS repo and navigates to the LSS/scripts/SV3 directory

source /global/common/software/desi/desi_environment.sh master
#add the LSS/py to python path; Ashley just has LSS in $HOME
PYTHONPATH=$PYTHONPATH:$HOME/LSS/py


#data, run separately for dark and bright time
#this only needs to be done once for each data release 
#it has been done for the fuji soft release, probably doesn't need to be done again but might want to double check
python mkCat_SV3.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec fuji --type dark --combd y
python mkCat_SV3.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec fuji --type bright --combd y

#Similarly, has been run for randoms for fuji already
#Randoms are intended to be parallized, so Ashley uses an interactive node via:
#salloc -N 1 -C haswell -t 04:00:00 --qos interactive --account desi
#and then runs the following
python mkCat_SV3_ran.py  --type dark  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec fuji --combr y 
python mkCat_SV3_ran.py  --type bright  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec fuji --combr y 

#combine healpix emline files
python mkCat_SV3.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec fuji --type dark --comb_emhp y

#the below script is run for this list of target types + notqso combinations in order to generate the "full" LSS catalogs, pre veto masks
#in this step, only unique targetid are kept, prioritizing those with an observation and then those with the greatest tsnr2
#targets at tileids/fiberid where none of the given type were assigned are masked
#if enhanced information on qso or ELG targets exists, it is added
#completeness statistics per tile grouping ('COMP_TILE') and per tileid/fiberid ('FRACZTILELOCID') are calculated
tps = ['LRG','BGS_ANY','BGS_BRIGHT','QSO','ELG','ELG_HIP','ELG_HIP','ELG']
notqsos = ['n','n','n','n','n','n','y','y'] #default for below is 'n', so only need the notqso argument for ELG_HIPnotqso
#Ashley does the list by hand one by one, so this is just pseudo code meant for illustration
for tp,notqso in zip(tps,notqsos):
    python mkCat_SV3.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld y --verspec fuji --notqso notqso 


#random
# go through per type
# make the "full" random files for each type
#this masks them using the same imaging mask bits as applied to targeting and also removes the tileids/fiberid where none of the given type were assigned (but an observation was requested)
#this expects to be on interactive node
#Ashley does the list by hand, so this is just pseudo code meant for illustration
for tp,notqso in zip(tps,notqsos):
    python mkCat_SV3_ran.py  --type tp  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec fuji --fullr y 

#apply veto mask column for LRGs
#this expects to be on interactive node
python getLRGmask.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --survey SV3 --verspec fuji --maxr 18 

#this applies vetos to both data and randoms (could put randoms in separate script and parallize)
for tp,notqso in zip(tps,notqsos):
    python mkCat_SV3.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --apply_veto y --verspec fuji --maxr 18 --notqso notqso
    
#to do randoms in parallel:
    python mkCat_SV3_ran.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fullr n --apply_veto y --verspec fuji --notqso notqso

    
#get all of the clustering catalogs
#getting clustering randoms is pretty quick, though one can make it quicker if the randoms are done in parallel
for tp,notqso in zip(tps,notqsos):
	python mkCat_SV3.py --type tp --verspec fuji --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --clus y --clusran y  --nz y --maxr 18 --notqso notqso

also do LRGs cut to main sample targeting cuts:
python mkCat_SV3.py --type LRG --verspec fuji --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --clus y --clusran y  --nz y --maxr 18 --ccut main
