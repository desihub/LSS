#LSS to run for Y1, starting from when there are new redshifts (e.g., himalayas)
#from the LSS/scripts/main directory

source /global/common/software/desi/desi_environment.sh main
export LSSCODE=<wherever_you_install_the_LSS_repo>
cd $LSSCODE
#if you have not installed it
git clone https://github.com/desihub/LSS.git
cd LSS
git pull
PYTHONPATH=$PYTHONPATH:$LSSCODE/LSS/py

#data, run separately for dark and bright time
#this script will merge the target info (already compiled from fiberassign files) with the spectroscopic info
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --prog dark --survey Y1
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --prog bright --survey Y1


#combine quasars:
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --prog dark --doqso y --dospec n --combpix n --survey Y1

#to combine the emission line files:
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --prog dark --mkemlin y --dospec n --combpix n --survey Y1


#the below script is run for this list of target types + notqso combinations in order to generate the "full" LSS catalogs, pre veto masks
#in this step, only unique targetid are kept, prioritizing those with an observation and then those with the greatest tsnr2
#targets at tileids/fiberid where none of the given type were assigned are masked
#if enhanced information on qso or ELG targets exists, it is addedca
#completeness statistics per tile grouping ('COMP_TILE') and per tileid/fiberid ('FRACZTILELOCID') are calculated
#running this script will go through all, takes ~1 hr
$LSSCODE/LSS/scripts/main/Y1_data_full.sh iron
#or go through one by one
for tp,notqso in zip(tps,notqsos):
    python $LSSCODE/LSS/scripts/main/mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld y --verspec iron --survey Y1 --notqso notqso


#random
#first, we go through bright and dark, making the mtl files per tile that fiberassign uses, then running fiberassign (to get potential assignments, which is the FAVAIL HDU in the fba files)
#after fiberassign is run, the potential assignments are combined per healpix
#does 18 randoms in parallel
#total amount of time is ~linear in the number of tiles to run
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/main/mkCat_main_Y1ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --type dark 
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/main/mkCat_main_Y1ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --type bright 

#if times out while still running fiberassign files, make sure to delete files with .tmp in the name

#then, we go through per type
#the "full" random files are made for each healpix
#this masks them using the same imaging mask bits as applied to targeting and also removes the tileids/fiberid where none of the given type were assigned (but an observation was requested)
for tp,notqso in zip(tps,notqsos):
    #on cori
   
    salloc -N 2 -C haswell -t 04:00:00 --qos interactive --account desi
    ./daily_main_randoms_type_2nodes_noveto.sh tp notqso
    #or run all
    ./daily_main_randoms_all_2nodes_noveto.sh
    #on perlmutter, enough memory so one node fine?
    #srun -N 1 -C cpu -t 04:00:00 -q interactive python mkCat_main_Y1ran_px.py  --type tp  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --rfa n --combhp n --combfull y --fullr y 

#this adds vetos to both data and randoms (could put randoms in separate script and parallize)
#only necessary for LRGs for now
python mkCat_main.py --type LRG --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --add_veto y --verspec himalayas --survey Y1
for tp,notqso in zip(tps,notqsos):
    python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --add_veto y --verspec himalayas --survey Y1 --notqso notqso
#can just run one random and then add --maxr 1

#fill randoms and apply vetoes to them:
for tp,notqso in zip(tps,notqsos):
	srun -N 1 -C cpu -t 04:00:00 -q interactive python scripts/main/mkCat_main_ran.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/   --verspec iron --survey Y1 --fillran y --apply_veto y

#make healpix maps based on randoms
for tp,notqso in zip(tps,notqsos):
	python scripts/main/mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --verspec iron --survey Y1  --mkHPmaps y

#this applies vetos to data 
for tp,notqso in zip(tps,notqsos):
    python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --apply_veto y --verspec daily --notqso notqso --maxr 0
#--maxr 0 stops the code from running any randoms



#get k+e
srun -N 1 -C cpu -t 04:00:00 -q interactive python mkCat_main.py --type BGS_BRIGHT --verspec himalayas --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --add_ke y --survey Y1 

#to get regressis and zfail weights
Y1_regressis_zfail.sh iron

#get regressis weights
#for most up to date version, need to start clean in cosmodesi environment and install seaborn
#source /global/common/software/desi/users/adematti/cosmodesi_environment.sh main
#PYTHONPATH=$PYTHONPATH:$HOME/LSS/py
#pip install seaborn
for tp,notqso in zip(tps,notqsos):
	python mkCat_main.py --type tp  --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --regressis y --add_regressis y --survey Y1 --verspec himalayas --notqso notqso
    
