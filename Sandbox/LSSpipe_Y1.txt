#LSS to run for Y1, starting from when there are new redshifts (e.g., himalayas)
#from the LSS/scripts/main directory

source /global/common/software/desi/desi_environment.sh main
export LSSCODE=<wherever_you_install_the_LSS_repo>
cd $LSSCODE
#if you have not installed it
git clone https://github.com/desihub/LSS.git
cd LSS
git pull
PYTHONPATH=$PYTHONPATH:$LSSCODE/LSS/py

#data, run separately for dark and bright time
#this script will merge the target info (already compiled from fiberassign files) with the spectroscopic info
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec iron --prog dark --survey Y1
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec iron --prog bright --survey Y1


#combine quasars:
#this got replaced by quasar only script, needs to be updated
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec iron --prog dark --doqso y --dospec n --combpix n --survey Y1

#to combine the emission line files:
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec iron --prog dark --mkemlin y --dospec n --combpix n --survey Y1


#the below script is run for this list of target types + notqso combinations in order to generate the "full" LSS catalogs, pre veto masks
#in this step, only unique targetid are kept, prioritizing those with an observation and then those with the greatest tsnr2
#targets at tileids/fiberid where none of the given type were assigned are masked
#if enhanced information on qso or ELG targets exists, it is addedca
#completeness statistics per tile grouping ('COMP_TILE') and per tileid/fiberid ('FRACZTILELOCID') are calculated
#running this script will go through all, takes ~1 hr
$LSSCODE/LSS/scripts/main/Y1_data_full.sh iron
#or go through one by one
for tp,notqso in zip(tps,notqsos):
    python $LSSCODE/LSS/scripts/main/mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld y --verspec iron --survey Y1 --notqso notqso


#random
#assuming target files for randoms were already produced for daily
#gets all potential assignments and collisions for each of 18 randoms, in sequence
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/getpotaY1_ran.py --prog BRIGHT
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/getpotaY1_ran.py --prog DARK

#OLD PROCESS FOR RANDOMS
#this process is much slower than new process , but the creation of target files for the randoms is still necessary to do first (though could also be sped up)
#first, we go through bright and dark, making the mtl files per tile that fiberassign uses, then running fiberassign (to get potential assignments, which is the FAVAIL HDU in the fba files)
#after fiberassign is run, the potential assignments are combined per healpix
#does 18 randoms in parallel
#total amount of time is ~linear in the number of tiles to run
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/main/mkCat_main_Y1ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --type dark 
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/main/mkCat_main_Y1ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --type bright 
#if times out while still running fiberassign files, make sure to delete files with .tmp in the name

#to combine with spec info and count tiles
srun -N 1 -C cpu -t 04:00:00 -q interactive python scripts/main/mkCat_main_ran.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec iron --type dark --combwspec y --counttiles y --survey Y1 
srun -N 1 -C cpu -t 04:00:00 -q interactive python scripts/main/mkCat_main_ran.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec iron --type bright --combwspec y --counttiles y --survey Y1 



#OLD
#then, we go through per type
#the "full" random files are made for each healpix
#this masks them using the same imaging mask bits as applied to targeting and also removes the tileids/fiberid where none of the given type were assigned (but an observation was requested)
for tp,notqso in zip(tps,notqsos):
    #on cori
   
    salloc -N 2 -C haswell -t 04:00:00 --qos interactive --account desi
    ./daily_main_randoms_type_2nodes_noveto.sh tp notqso
    #or run all
    ./daily_main_randoms_all_2nodes_noveto.sh
    #on perlmutter, enough memory so one node fine?
    #srun -N 1 -C cpu -t 04:00:00 -q interactive python mkCat_main_Y1ran_px.py  --type tp  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec himalayas --rfa n --combhp n --combfull y --fullr y
    
#Current:
for tp,notqso in zip(tps,notqsos):
	srun -N 1 -C cpu -t 04:00:00 -q interactive python scripts/main/mkCat_main_ran.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec iron --type tp --combwspec n --fullr y --survey Y1 

#this adds vetos to both data and randoms (could put randoms in separate script and parallize)
#only necessary for LRGs for now
python mkCat_main.py --type LRG --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --add_veto y --verspec himalayas --survey Y1
for tp,notqso in zip(tps,notqsos):
    python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --add_veto y --verspec himalayas --survey Y1 --notqso notqso
#can just run one random and then add --maxr 1

#fill randoms and apply vetoes to them:
for tp,notqso in zip(tps,notqsos):
	srun -N 1 -C cpu -t 04:00:00 -q interactive python scripts/main/mkCat_main_ran.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/   --verspec iron --survey Y1 --fillran y --apply_veto y

#make healpix maps based on randoms
for tp,notqso in zip(tps,notqsos):
	python scripts/main/mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --verspec iron --survey Y1  --mkHPmaps y

#add bitweights to data before applying vetos
python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --add_bitweight y --verspec iron --survey Y1 --notqso notqso --version v#

#this applies vetos to data and randoms (no need for interactive node and use --maxr 0 if you don't want to run randoms
for tp,notqso in zip(tps,notqsos):
    srun -N 1 -C cpu -t 04:00:00 -q interactive python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --apply_veto y --verspec iron --survey Y1 --notqso notqso --version v#

#add completeness info per tiles group to randoms, needs to be done after vetos applied to data
for tp,notqso in zip(tps,notqsos):
	srun -N 1 -C cpu -t 04:00:00 -q interactive python scripts/main/mkCat_main_ran.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/   --verspec iron --survey Y1 --add_tl y --version <LSSversion> 
	#doing in chunks of 6 seems to be robust and the multiprocessing has been hardcoded for that
	#i/o goes slow on node sometimes and this computing is fast, so the following might work better:
	python scripts/main/mkCat_main_ran.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/   --verspec iron --survey Y1 --add_tl y --par n

#apply map based veto to data and random; if applying this, make sure to subsequently use --use_map_veto _HPmapcut as an option below
for tp,notqso in zip(tps,notqsos):
    srun -N 1 -C cpu -t 04:00:00 -q interactive python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --apply_map_veto y --verspec iron --survey Y1 --notqso notqso --version v#

#this add the completeness info after the map based veto to randoms
srun -N 1 -C cpu -t 04:00:00 -q interactive python scripts/main/mkCat_main.py --type LRG --basedir /global/cfs/cdirs/desi/survey/catalogs/   --verspec iron --survey Y1 --add_tlcomp y --version v0.6 --fulld n

#get rest frame from fastspecfit
python mkCat_main.py --type BGS_BRIGHT --verspec iron --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --add_fs y --survey Y1 --version <LSSversion> 

#NO LONGER DEFAULT
#get k+e
srun -N 1 -C cpu -t 04:00:00 -q interactive python mkCat_main.py --type BGS_BRIGHT --verspec himalayas --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --add_ke y --survey Y1 

#to get regressis and zfail weights
source /global/common/software/desi/users/adematti/cosmodesi_environment.sh main
python $LSSCODE/LSS/scripts/main/mkCat_main.py --type BGS_BRIGHT-21.5 --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --regressis y --add_regressis y --verspec iron --version v0.6 --add_weight_zfail y --survey Y1  --use_map_veto _HPmapcut
#script to do all needs editing Y1_regressis_zfail.sh iron

#get regressis weights
#source /global/common/software/desi/users/adematti/cosmodesi_environment.sh main
#PYTHONPATH=$PYTHONPATH:$HOME/LSS/py
for tp,notqso in zip(tps,notqsos):
	python mkCat_main.py --type tp  --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --regressis y --add_regressis y --survey Y1 --verspec iron --imsys_zbin y (--notqso notqso) 
    
#get n(z) and fkp weights (not really necessary; depricated)
#python scripts/main/mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --verspec iron --#survey Y1  --FKPfull y --nzfull y

#apply blinding and get blinded clustering catalogs
module swap pyrecon/main pyrecon/mpi
srun -n 128 python scripts/main/apply_blinding_main_fromfile_fcomp.py --type tp --basedir_out /global/cfs/cdirs/desi/survey/catalogs/Y1/LSS/iron --version v0.1 --baoblind y --mkclusdat y --mkclusran y --maxr 18 --dorecon y --rsdblind y --fnlblind y --getFKP y

#get unblinded clustering catalogs, in all of N, S, NGC, SGC regions
python scripts/main/mkCat_main.py --type BGS_BRIGHT --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --survey Y1 --verspec iron --version v0.6 --clusd y --clusran y --NStoGC y --nz y --par n --resamp y

