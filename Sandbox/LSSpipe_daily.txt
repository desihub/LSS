#LSS to run with every MTL update
#from the LSS/scripts/main directory

source /global/common/software/desi/desi_environment.sh main
export LSSCODE=<wherever_you_install_the_LSS_repo>
cd $LSSCODE
#if you have not installed it
git clone https://github.com/desihub/LSS.git
cd LSS
git pull
PYTHONPATH=$PYTHONPATH:$LSSCODE/LSS/py
module swap fiberassign/5.0.0

#if you need to delete temporary fiberassign files
rm /global/cfs/cdirs/desi/survey/catalogs//main/LSS/random*/*tmp*

#data, run separately for dark and bright time
#this script first goes through the fiberassign files, first combining all of the information together split by healpix
#all "potential assignments" are kept, meaning each targetid will appear many times
#then, it collects the new spec data for any archived/zdone tiles (incl. the info in the zmtl files used to decide mtl updates)
#then, it combines data from the healpix files selecting on given target types + notqso combinations; notqso means that targets passing the qso selection are removed
#after combining, the sample is matched to the spec data based on 'TARGETID','LOCATION','TILEID','TILELOCID'
#information on the unique groupings of tileids and tilelocid are determined (along with the number of tiles) and this information is joined by a TARGETID match
#It will take awhile, >~ 1 hour for dark time and ~> 30 min for bright, with some variance depending on the number of new tiles (test run with 15 new dark and 41 new bright on Perlmutter)
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark 
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog bright 


#combine quasars:
#run time is linear in number of tiles to process
#some number, missing files not necessary for redrock pipeline but necessary for quasar catalog, will fail
#if number of failures is > 4 (number failing for Y1), notify people so we can keep track
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark --doqso y --dospec n --combpix n

#to make the emission line files for each tile
#run time is ~linear in number of tiles to process, < 100 should take < 1 hr
srun -N 1 -C cpu -t 02:00:00 -q interactive python $LSSCODE/LSS/scripts/mkemlin.py

#to combine the emission line files:
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark --mkemlin y --dospec n --combpix n


#the below script is run for this list of target types + notqso combinations in order to generate the "full" LSS catalogs, pre veto masks
#in this step, only unique targetid are kept, prioritizing those with an observation and then those with the greatest tsnr2
#targets at tileids/fiberid where none of the given type were assigned are masked
#if enhanced information on qso or ELG targets exists, it is addedca
#completeness statistics per tile grouping ('COMP_TILE') and per tileid/fiberid ('FRACZTILELOCID') are calculated
#running this script will go through all, takes ~1 hr
$LSSCODE/LSS/scripts/main/daily_main_data_full.sh
#or go through one by one
for tp,notqso in zip(tps,notqsos):
    python $LSSCODE/LSS/scripts/main/mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld y --verspec daily --notqso notqso


#random
#first, we go through bright and dark, making the mtl files per tile that fiberassign uses, then running fiberassign (to get potential assignments, which is the FAVAIL HDU in the fba files)
#after fiberassign is run, the potential assignments are combined per healpix
#does 18 randoms in parallel
#total amount of time is ~linear in the number of tiles to run
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/main/mkCat_main_ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --type dark --ranmtl y 
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/main/mkCat_main_ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --type bright --ranmtl y 

#if times out while still running fiberassign files, make sure to delete files with .tmp in the name

#then, we go through per type
#the "full" random files are made for each healpix
#this masks them using the same imaging mask bits as applied to targeting and also removes the tileids/fiberid where none of the given type were assigned (but an observation was requested)

$LSSCODE/LSS/scripts/main/daily_main_randoms_all_noveto.sh

#this adds vetos to both data and randoms (could put randoms in separate script and parallize)
#only necessary for LRGs for now
#just do one random (can do all, but not necessary for most basic tasks and also easy to do later)
python $LSSCODE/LSS/scripts/main/mkCat_main.py --type LRG --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --add_veto y --verspec daily --maxr 1


#this applies vetos to both data and randoms (could put randoms in separate script and parallize)
#pay attention to message "sum of 1/FRACZ_TILELOCID, 1/COMP_TILE, and length of input; should approximately match" and check if numbers look good

$LSSCODE/LSS/scripts/main/daily_main_data1ran_vetos.sh

#Numbers for each tracer as of Oct 18th 2022, if relative match is much different, report
ELG
13710917.0 13912449 13919838
ELG_LOP
10706974.0 10860317.0 10866073
ELG_LOPnotqso
8722851.0 8735241.0 8741168
LRG
3511046.0 3525933.0 3526807
BGS_BRIGHT
6462164.0 6465287.0 6465332
BGS_ANY
10890090.0 10944202.0 10944275

#Now, the stats can be updated (and what is there should be good any other tests)
#first run like

python $LSSCODE/LSS/scripts/main/write_daily_stats.py

#This will create/add to a file named temp.txt, with the most recent stats at the bottom.

cat temp.txt

#Check that the numbers make sense compared to https://data.desi.lbl.gov/desi/survey/catalogs/main/LSS/daily/LSScats/stats.txt
#(e.g., the numbers should get bigger)
#For instance, you might forget to combine the emission line data, and then the ELG redshifts might not grow, then after

python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark --mkemlin y --dospec n --combpix n

(same as should have been run earlier)
you could run

$LSSCODE/LSS/scripts/main/daily_main_dataELG_full.sh
$LSSCODE/LSS/scripts/main/daily_main_dataELG_vetos.sh

If all looks good in temp.txt

python $LSSCODE/LSS/scripts/main/write_daily_stats.py --add y

