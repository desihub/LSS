#LSS to run with every MTL update
#from the LSS/scripts/main directory

source /global/common/software/desi/desi_environment.sh master
PYTHONPATH=$PYTHONPATH:$HOME/LSS/py
module swap fiberassign/5.0.0

#if you need to delete temporary fiberassign files
rm /global/cfs/cdirs/desi/survey/catalogs//main/LSS/random*/*tmp*

#data, run separately for dark and bright time
#this script first goes through the fiberassign files, first combining all of the information together split by healpix
#all "potential assignments" are kept, meaning each targetid will appear many times
#then, it collects the new spec data for any archived/zdone tiles (incl. the info in the zmtl files used to decide mtl updates)
#then, it combines data from the healpix files selecting on given target types + notqso combinations; notqso means that targets passing the qso selection are removed
#for this combination, only data at the tileids/fiberids that were considered good are kept
tps = ['LRG','QSO','ELG','ELG_LOP','ELG_LOP','BGS_ANY','BGS_BRIGHT']
notqsos = ['n','n','n','n','y','n','n']
#after combining, the sample is matched to the spec data based on 'TARGETID','LOCATION','TILEID','TILELOCID'
#information on the unique groupings of tileids and tilelocid are determined (along with the number of tiles) and this information is joined by a TARGETID match
python combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark
python combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog bright


#combine quasars:
python combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark --doqso y --dospec n --combpix n

#to make the emission line files for each tile
on interactive node
srun -N 1 -C cpu -t 01:00:00 -q interactive python mkemlin.py

to combine the emission line files:
python combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark --mkemlin y --dospec n --combpix n


#the below script is run for this list of target types + notqso combinations in order to generate the "full" LSS catalogs, pre veto masks
#in this step, only unique targetid are kept, prioritizing those with an observation and then those with the greatest tsnr2
#targets at tileids/fiberid where none of the given type were assigned are masked
#if enhanced information on qso or ELG targets exists, it is added
#completeness statistics per tile grouping ('COMP_TILE') and per tileid/fiberid ('FRACZTILELOCID') are calculated
for tp,notqso in zip(tps,notqsos):
    python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld y --verspec daily --notqso notqso


#random
#first, we go through bright and dark, making the mtl files per tile that fiberassign uses, then running fiberassign (to get potential assignments, which is the FAVAIL HDU in the fba files)
#after fiberassign is run, the potential assignments are combined per healpix
srun -N 1 -C haswell -c 64 -t 04:00:00 -q interactive python mkCat_main_ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --type dark --ranmtl y 
srun -N 1 -C haswell -c 64 -t 04:00:00 -q interactive python mkCat_main_ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --type bright --ranmtl y 

#if times out while still running fiberassign files, make sure to delete files with .tmp in the name

#then, we go through per type
#the "full" random files are made for each healpix
#this masks them using the same imaging mask bits as applied to targeting and also removes the tileids/fiberid where none of the given type were assigned (but an observation was requested)
for tp,notqso in zip(tps,notqsos):
    salloc -N 2 -C haswell -t 04:00:00 --qos interactive --account desi
    ./daily_main_randoms_type_2nodes_noveto.sh tp notqso
    #or run all
    ./daily_main_randoms_all_2nodes_noveto.sh
    #srun -N 1 -C haswell -c 64 -t 04:00:00 -q interactive python mkCat_main_ran_px.py  --type tp  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --rfa n --combhp n --combfull y --fullr y 
#if data model is stable since the last updates, add to the arguments
    --refullr n

#apply veto mask column for LRGs
#!don't need this anymore!
#python getLRGmask.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --survey main --verspec daily --maxr 18

#this adds vetos to both data and randoms (could put randoms in separate script and parallize)
for tp,notqso in zip(tps,notqsos):
    python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --add_veto y --verspec daily --notqso notqso


#this applies vetos to both data and randoms (could put randoms in separate script and parallize)
for tp,notqso in zip(tps,notqsos):
    python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --apply_veto y --verspec daily --notqso notqso
    
#to do randoms in parallel:
    python mkCat_main_ran.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fullr n --apply_veto y --verspec daily --notqso notqso
